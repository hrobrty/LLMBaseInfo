### 1，AGI知识体系介绍

#### 1，什么是AI 
主流观点是基于机器学习，神经网络是Ai，基于规则搜索不是Ai
用好Ai的核心心法：**把Ai当人看，指令具体，信息丰富，减少歧义**

#### 2，什么是AGI
AGI全称：artificial general intelligence 是通用人工智能
- **狭义Ai：** 专注于特定任务的Ai，任务范围明确
- **广义Ai：** 能够处理广泛非物理任务的Ai，包括学习新技能能力

#### 3，AGI时代社会分层
**(1)Ai使用者（人人都用，不稀奇）**  
类比移动互联网，可以使用各种软件
**(2)Ai产品开发者（技术门槛中等）**  
类比移动互联网，开发各种软件
**(3)基础模型相关（技术难门槛高）**  
平台开发gpt，llama等
底层系统开发，需要海量数据，算力和技术人才

#### 4，AGI时代核心能力  
努力做3懂人才  
**(1)懂业务：懂用户，客户，需求，市场，运营，商业模式**  
懂用户：知道目标用户是谁，他们的习惯和痛点  
懂客户：明白付钱的人(可能是老板或公司)想要什么  
懂需求：能分清用户的真实需求  
懂市场：了解行业趋势，有点到线，由线到面  
懂运营：知道产品怎么推广和留住用户  
懂商业模式：清楚怎么赚钱：1，收会员费；2，卖数据分析服务；3，免费引流卖广告  
**(2)懂Ai：知道能做什么，不能做什么，怎么做更快，更好，更便宜**  
能做什么：认图，写作，做数据分析  
不能做什么：清楚ai局限，不能取代人做创意设计和处理复杂情感  
更快：懂得用县城工具或模型。使用开源框架改改就能用  
更好：回调参数，改数据，让ai结果更准。比如：客服机器人别回答驴群不对马嘴  
更便宜：能省资源。选择合适模型匹配相应任务，不浪费算力。
**(3)懂编程：实现一个符合业务需求的产品**  
会写代码。把需求落地。比喻：客户要求销售预测工具，能用python写一个工具，介绍ai模型，吐出结果给老板看。还得简单好用，别让用户学半天。  

#### 5，目前行业共识  
2个确定，1个不确定  
(1)确定未来：ai必然改变世界  
(2)确定进入：想获得红利，必须马上进入  
(3)不确定落地：解决什么问题，用什么技术路线，产品策略是什么，确定性都不高  

#### 6，AGI落地场景思路
(1) 从熟悉的领域入手  
(2) 找“文本入，文本出”的场景  
(3) 别求大而全，将任务拆解，先解决任务小场景  
(4) 让ai学习最厉害员工能力，辅助其它员工，降本增效  

#### 7，大模型是什么及如何产生结果
LLM（large lauguage model）大预言模型型是人工智能领域的一种技术
基于深度学习（特别是神经网络领域中的Transformar架构），通过海量文本数据训练，能够理解和生成自然语言的AI系统
那LLM如何生成结果
通俗原理：ai只是根据上下文猜下一个词（概率）
略深点的原理：训练和推理是大模型的两个和性过程（训练就是学，推理就是用）
用不严密单通俗的语言描述：
**（1）训练**
大模型阅读了人类说过的话，就是机器学习
训练过程中会把说过话中不同token出现概率存入神经网络文件，保存的数据就是参数，也叫权重
LLM通常有十几亿到上千亿参数（parameters），这些参数指通过训练调整的，决定了模型对语言的理解和生成能力。（deepseek-R1有671B个参数）
**（2）推理**
给推理程序若干个token(prompt),程序会加载大模型权重算出下一个token是什么
用生成的token再加上上下文，就能继续生成下一个token

#### 8，Token是什么
在LLM中，Token是处理文本的基本单位，不是简单的单词或字符，而是经过Tokenizer（分词器）处理后的文本片段
Token长度和内容取决于具体的分词策略和模型设计

#### 9，LLM生成机制内核
深度学习框架：Transformer,RWKV，Mamba
其中Transformer框架是主流，但也不是最新的了，谷歌开源，最流行，几乎所有大模型都用它
**Transformer**
由谷歌2017年提出的架构《Attention is All You Need》,彻底改变了自然语言(NPL)处理领域，放弃了传统的循环神经网络(RNN)，改用* 自注意力机制(self-attention)*来并行处理序列数据
**RWKV，Mamba（Recurrent weighted key-value）**
是一种较新的架构，结合了Transformer和RNN的优点，试图解决Transformer的效率问题。由社区驱动开发，非Google主导
**Mamba**
是20232024年提出的新架构，旨在取代Transformer，尤其针对长序列和效率问题。它基于结构化状态空间模型（Structure State Space Models, SSM）
目前只有Transformer符合**Scalling—Law**（是深度学习领域的一种经验规律
三个影响模型性能的因素：  
1，模型参数量（更大的网络）  
2，训练数据量（跟多文本） 
3，计算资源（跟多算力1） 

#### 10，LLM应用业务架构  
（1）AI Embedding模式，人主导，中间某个环节由AI参与，如人脸识别  
（2）AI Copilot模式， ai和人主导，每个环节ai参与，如搜索引擎  
（3）Agent模式，人把事情丢给Ai，Ai自己规划自己去做，如Manus,Mcp  

#### 11,应用技术架构  
4中主流应用技术架构，门槛低，天花板高  
**（1）指令工程 Prompt Engineeing**  
使用场景：知识问答，数据分析，写作，编码，文本加工  
核心思想：构造一个有效的且正确的prompt：指令具体，信息丰富，没有歧义  
技术架构图如下：  
![架构图](https://github.com/NanGePlus/LLMsBasisDevelopment/raw/main/01_BasicIntro/01_AGI%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E4%BB%8B%E7%BB%8D/03.png)

**(2)给大模型知识库RAG**  
使用场景：智能知识库，智能诊断，数字分身，带例子的prompt等  
核心思想：人找知识会查资料，；llm找知识会查找向量数据库（相似度检索） 
技术架构图如下：  
![架构图](https://github.com/NanGePlus/LLMsBasisDevelopment/raw/main/01_BasicIntro/01_AGI%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E4%BB%8B%E7%BB%8D/04.png)

**(3)让大模型对接外界Function Calling**  
使用场景：智能助手，下一代搜索引擎，机器人，Agent等  
基本过程：大模型调用内外部工具处理问题  
Agent使用json格式反向提出要求，应用大模型的规则性原则  
技术架构图如下：  
![架构图](https://github.com/NanGePlus/LLMsBasisDevelopment/raw/main/01_BasicIntro/01_AGI%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E4%BB%8B%E7%BB%8D/05.png)

**（4）让大模型深度理解Fine-tuning**  
使用场景：智能知识库，智能诊断，数字分身  
核心思想：人阅读背诵，理解资料。LLM进行增强训练学习。  
基础过程：给基础大模型增加参数(补充垂直领域知识)，就大模型进行轻量化微调，Fine-tuning全过程都在使用工具（不需要造轮子）  
技术结果图如下：  
![架构图](https://github.com/NanGePlus/LLMsBasisDevelopment/raw/main/01_BasicIntro/01_AGI%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E4%BB%8B%E7%BB%8D/06.png)

与RAG解决场景类似，差别：
RAG查询完资料就会忘记，下次需要重新再查  
微调后的垂直领域大模型(Fine-tuning)是将知识记住，可立即给结果  
类比：开卷考试两种方案  
方案一：RAG会节省算力，但耗时长  
方案二：Fine-tuning会消耗算力，但及时反馈  

#### 12，技术路线选择
面对一个需求如何开始，如何选择技术方案，常用思路如下：  
![结构图](https://github.com/NanGePlus/LLMsBasisDevelopment/raw/main/01_BasicIntro/01_AGI%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E4%BB%8B%E7%BB%8D/07.png)
前两步最重要，耗时最高  
值得尝试Fine-tuning情况：  
（1）提高模型输出稳定性
（2）提高模型相应速度
（3）用户量大，降低推理成本
（4）需要私有化部署
